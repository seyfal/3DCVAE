{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3102951/599858328.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/ssulta24/Desktop/VCAE_new/wandb/run-20240830_155434-okinkke3/files/best_model.pth', map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CVAE3D:\n\tMissing key(s) in state_dict: \"encoder.0.0.weight\", \"encoder.0.0.bias\", \"encoder.1.0.weight\", \"encoder.1.0.bias\", \"encoder.2.0.weight\", \"encoder.2.0.bias\", \"fc_mu.weight\", \"fc_mu.bias\", \"fc_var.weight\", \"fc_var.bias\", \"decoder_input.weight\", \"decoder_input.bias\", \"decoder.0.0.weight\", \"decoder.0.0.bias\", \"decoder.1.0.weight\", \"decoder.1.0.bias\", \"decoder.2.0.weight\", \"decoder.2.0.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.9.weight\", \"encoder.9.bias\", \"encoder.11.weight\", \"encoder.11.bias\", \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"decoder.5.weight\", \"decoder.5.bias\", \"decoder.7.weight\", \"decoder.7.bias\", \"decoder.9.weight\", \"decoder.9.bias\", \"decoder.11.weight\", \"decoder.11.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m CVAE3D(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m240\u001b[39m), latent_dim\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_dims\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dims\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(torch.load('/home/ssulta24/Desktop/VCAE_new/wandb/run-20240823_154222-zcnsqis6/files/final_model.pth', map_location=device))\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ssulta24/Desktop/VCAE_new/wandb/run-20240830_155434-okinkke3/files/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# model = train_model(config)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.12/site-packages/torch/nn/modules/module.py:2568\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2560\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2561\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2562\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2563\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2564\u001b[0m             ),\n\u001b[1;32m   2565\u001b[0m         )\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2569\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[1;32m   2572\u001b[0m     )\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CVAE3D:\n\tMissing key(s) in state_dict: \"encoder.0.0.weight\", \"encoder.0.0.bias\", \"encoder.1.0.weight\", \"encoder.1.0.bias\", \"encoder.2.0.weight\", \"encoder.2.0.bias\", \"fc_mu.weight\", \"fc_mu.bias\", \"fc_var.weight\", \"fc_var.bias\", \"decoder_input.weight\", \"decoder_input.bias\", \"decoder.0.0.weight\", \"decoder.0.0.bias\", \"decoder.1.0.weight\", \"decoder.1.0.bias\", \"decoder.2.0.weight\", \"decoder.2.0.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.6.weight\", \"encoder.6.bias\", \"encoder.9.weight\", \"encoder.9.bias\", \"encoder.11.weight\", \"encoder.11.bias\", \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"decoder.5.weight\", \"decoder.5.bias\", \"decoder.7.weight\", \"decoder.7.bias\", \"decoder.9.weight\", \"decoder.9.bias\", \"decoder.11.weight\", \"decoder.11.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\". "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import umap\n",
    "\n",
    "from anomaly_detection.models.cvae3d_flex import CVAE3D\n",
    "# from anomaly_detection.models.cvae3d import CVAE3D\n",
    "from anomaly_detection.data.data_loader import get_data_loader\n",
    "from anomaly_detection.config.config_handler import get_config\n",
    "from anomaly_detection.training.train import train_model\n",
    "\n",
    "# Helper function to convert numpy array to base64 image\n",
    "def np_image_to_base64(im_matrix):\n",
    "    im = Image.fromarray(im_matrix)\n",
    "    buffer = io.BytesIO()\n",
    "    im.save(buffer, format=\"png\")\n",
    "    encoded_image = base64.b64encode(buffer.getvalue()).decode()\n",
    "    im_url = \"data:image/png;base64, \" + encoded_image\n",
    "    return im_url\n",
    "\n",
    "# Load your configuration\n",
    "config = get_config('/home/ssulta24/Desktop/VCAE_new/anomaly_detection/config/config.yaml')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load your pre-trained model\n",
    "model = CVAE3D(input_shape=(24, 24, 240), latent_dim=config['latent_dim'], hidden_dims=config['hidden_dims']).to(\"cuda:1\")\n",
    "# model.load_state_dict(torch.load('/home/ssulta24/Desktop/VCAE_new/wandb/run-20240823_154222-zcnsqis6/files/final_model.pth', map_location=device))\n",
    "model.load_state_dict(torch.load('/home/ssulta24/Desktop/VCAE_new/wandb/run-20240830_155434-okinkke3/files/best_model.pth', map_location=device))\n",
    "\n",
    "# model = train_model(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;20mWARNING | Hyperspy | `signal_type='EELS'` not understood. See `hs.print_known_signal_types()` for a list of installed signal types or https://github.com/hyperspy/hyperspy-extensions-list for the list of all hyperspy extensions providing signals. (hyperspy.io:744)\u001b[0m\n",
      "\u001b[33;20mWARNING | Hyperspy | `signal_type='EELS'` not understood. See `hs.print_known_signal_types()` for a list of installed signal types or https://github.com/hyperspy/hyperspy-extensions-list for the list of all hyperspy extensions providing signals. (hyperspy.io:744)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting random samples: 100%|██████████| 64/64 [00:00<00:00, 152.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "[t-SNE] Computing 63 nearest neighbors...\n",
      "[t-SNE] Indexed 64 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 64 samples in 0.063s...\n",
      "[t-SNE] Computed conditional probabilities for sample 64 / 64\n",
      "[t-SNE] Mean sigma: 0.000002\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 113.118515\n",
      "[t-SNE] KL divergence after 1000 iterations: 1.010657\n"
     ]
    }
   ],
   "source": [
    "# Get your data loader\n",
    "data_loader = get_data_loader(config)\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_samples = len(data_loader.dataset)\n",
    "\n",
    "# Randomly select 5000 indices\n",
    "max_samples = 64\n",
    "random_indices = random.sample(range(total_samples), min(max_samples, total_samples))\n",
    "\n",
    "# Collect embeddings and original data\n",
    "embeddings = []\n",
    "original_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(random_indices, desc=\"Collecting random samples\"):\n",
    "        # Get the specific sample from the dataset\n",
    "        x = data_loader.dataset[idx]\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x)\n",
    "        x = x.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        \n",
    "        mean, _ = model.encode(x)\n",
    "        embeddings.append(mean.cpu().numpy())\n",
    "        original_data.append(x.cpu().numpy())\n",
    "embeddings = np.vstack(embeddings)\n",
    "original_data = np.vstack(original_data)\n",
    "\n",
    "print(len(embeddings))\n",
    "\n",
    "# Randomly select 5000 samples\n",
    "if len(embeddings) > max_samples:\n",
    "    indices = random.sample(range(len(embeddings)), max_samples)\n",
    "    embeddings = embeddings[indices]\n",
    "    original_data = original_data[indices]\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=3, random_state=42, verbose=1)\n",
    "embeddings = tsne.fit_transform(embeddings) # Takes exponentially more time to compute\n",
    "\n",
    "# # Perform UMAP  \n",
    "# reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "# embeddings = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "# Create 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=embeddings[:, 0],\n",
    "    y=embeddings[:, 1],\n",
    "    z=embeddings[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=np.sum(original_data, axis=(1, 2, 3, 4)),\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"3D UMAP of Latent Space (Epoch {1})\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"UMAP 1\",\n",
    "        yaxis_title=\"UMAP 2\",\n",
    "        zaxis_title=\"UMAP 3\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    hoverinfo=\"none\",\n",
    "    hovertemplate=None,\n",
    ")\n",
    "\n",
    "# Create Dash app\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"graph-3d-plot\", figure=fig, clear_on_unhover=True),\n",
    "    dcc.Tooltip(id=\"graph-tooltip\", direction='bottom'),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"graph-tooltip\", \"show\"),\n",
    "    Output(\"graph-tooltip\", \"bbox\"),\n",
    "    Output(\"graph-tooltip\", \"children\"),\n",
    "    Input(\"graph-3d-plot\", \"hoverData\"),\n",
    ")\n",
    "\n",
    "def display_hover(hoverData):\n",
    "    if hoverData is None:\n",
    "        return False, no_update, no_update\n",
    "\n",
    "    hover_data = hoverData[\"points\"][0]\n",
    "    bbox = hover_data[\"bbox\"]\n",
    "    num = hover_data[\"pointNumber\"]\n",
    "\n",
    "    # Create EELS image\n",
    "    eels_data = original_data[num].squeeze().sum(axis=-1)\n",
    "    eels_data = (eels_data - eels_data.min()) / (eels_data.max() - eels_data.min())\n",
    "    eels_image = (eels_data * 255).astype(np.uint8)\n",
    "    im_url = np_image_to_base64(eels_image)\n",
    "\n",
    "    children = [\n",
    "        html.Div([\n",
    "            html.Img(\n",
    "                src=im_url,\n",
    "                style={\"width\": \"100px\", 'display': 'block', 'margin': '0 auto'},\n",
    "            ),\n",
    "            html.P(f\"Sample {num}\", style={'font-weight': 'bold', 'text-align': 'center'})\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    return True, bbox, children\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(jupyter_mode=\"external\", jupyter_height=2000, jupyter_width=\"200%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
